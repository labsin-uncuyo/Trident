# John Scott LLM - NPC con IntegraciÃ³n de LLM

## DescripciÃ³n

`john_scott_llm` es un agente NPC (Non-Player Character) para GHOSTS que utiliza un Large Language Model (LLM) para generar consultas SQL dinÃ¡micamente. A diferencia de `john_scott_dummy` que usa queries hardcodeadas, este agente genera comportamiento adaptativo usando IA.

## Arquitectura de IntegraciÃ³n LLM + GHOSTS

### Flujo de Trabajo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Inicio del Contenedor (ghosts_driver)                    â”‚
â”‚    - Lee JOHN_SCOTT_MODE desde variables de entorno         â”‚
â”‚    - Si MODE=llm â†’ ejecuta generate_timeline.sh             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. GeneraciÃ³n de Timeline (generate_timeline_llm.py)        â”‚
â”‚    - Lee configuraciÃ³n LLM desde env vars                   â”‚
â”‚    - Define tareas en lenguaje natural                      â”‚
â”‚    - Por cada tarea: llama al LLM para generar SQL          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. LLM API Call (OpenAI-compatible)                         â”‚
â”‚    Input: "Check all employees in Engineering department"   â”‚
â”‚    Context: Schema de la base de datos (tables, columns)    â”‚
â”‚    Output: "SELECT * FROM employees WHERE department=       â”‚
â”‚            'Engineering' LIMIT 10;"                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. ConstrucciÃ³n del Timeline JSON                           â”‚
â”‚    - Cada query SQL â†’ evento en el timeline                 â”‚
â”‚    - Incluye comandos SSH con queries generadas             â”‚
â”‚    - Guarda: timeline_john_scott_llm.json                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. GHOSTS Client Ejecuta Timeline                           â”‚
â”‚    - Lee timeline.json                                       â”‚
â”‚    - Ejecuta comandos SSH a 172.30.0.10 (compromised)       â”‚
â”‚    - Cada comando ejecuta psql con la query generada        â”‚
â”‚    - Loop continuo con delays configurados                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Clave

#### 1. **generate_timeline_llm.py**
Script Python que genera el timeline dinÃ¡micamente usando LLM.

**Responsabilidades:**
- Conectar con API de LLM (OpenAI-compatible)
- Enviar prompts con contexto del schema de la base de datos
- Generar queries SQL basadas en descripciones de tareas
- Construir timeline JSON para GHOSTS
- Manejar fallbacks si el LLM falla

**ConfiguraciÃ³n LLM:**
```python
LLM_API_KEY = os.getenv('OPENAI_API_KEY')
LLM_BASE_URL = os.getenv('OPENAI_BASE_URL').rstrip('/')  # Elimina trailing slash
LLM_MODEL = os.getenv('LLM_MODEL', 'qwen3-coder')
LLM_TEMPERATURE = float(os.getenv('LLM_TEMPERATURE', '0.7'))
```

**Prompt del Sistema:**
```python
SYSTEM_PROMPT = f"""You are a PostgreSQL query generator for John Scott, a Senior Developer.

{DATABASE_SCHEMA}  # Incluye estructura completa de tablas

Generate ONLY the PostgreSQL query without any explanation, markdown formatting, or additional text.
The query must be a valid PostgreSQL statement that can be executed directly.
Vary the queries - include SELECT, JOIN, GROUP BY, COUNT, AVG, SUM operations.
Keep queries realistic for a developer's daily work.
Respond with ONLY the SQL query, nothing else."""
```

#### 2. **ConexiÃ³n SSH Hardcodeada**
Como solicitaste, la conexiÃ³n SSH estÃ¡ fija en el cÃ³digo:

```python
SSH_TARGET = "labuser@172.30.0.10"
SSH_KEY = "/root/.ssh/id_rsa"
SSH_OPTIONS = "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
```

#### 3. **Tareas en Lenguaje Natural**
Las tareas se definen como descripciones, no como SQL:

```python
tasks = [
    "Check all employees in the Engineering department",
    "Find the average salary by department",
    "List all active projects with their team sizes",
    "Get recent hires from the last year",
    "Show department budgets and managers",
    "Find employees working on multiple projects",
    "Calculate total project hours by employee",
    "List departments with more than 10 employees"
]
```

El LLM convierte cada tarea en una query SQL apropiada.

#### 4. **Integration con GHOSTS**
El timeline generado tiene el formato estÃ¡ndar de GHOSTS:

```json
{
  "Id": "d531df3a-c946-4a53-beac-57d70c97d799",
  "Status": "Active",
  "TimeLineHandlers": [{
    "HandlerType": "Command",
    "Loop": true,
    "TimeLineEvents": [
      {
        "Command": "ssh -i /root/.ssh/id_rsa labuser@172.30.0.10 \"psql -h 172.30.0.3 -U laboratorio -d employees -c \\\"SELECT * FROM employees WHERE department='Engineering'\\\"\"",
        "DelayAfter": 30000,
        "DelayBefore": 15000
      }
    ]
  }]
}
```

## Variables de Entorno

Definidas en `.env` y pasadas por `docker-compose.yml`:

```bash
# Proveedor LLM (e-INFRA CZ o compatible OpenAI)
OPENAI_API_KEY=YOUR_API_KEY
OPENAI_BASE_URL=https://chat.ai.e-infra.cz/api
LLM_MODEL=qwen3-coder
LLM_TEMPERATURE=0.7

# Modo de operaciÃ³n (dummy o llm)
JOHN_SCOTT_MODE=llm  # Se define al ejecutar docker compose
```

## Uso

### 1. Levantar en Modo LLM

```bash
# Reconstruir imagen con los cambios
docker compose build ghosts_driver

# Levantar en modo LLM
JOHN_SCOTT_MODE=llm docker compose up -d ghosts_driver
```

### 2. Verificar Timeline Generado

```bash
# Ver logs de generaciÃ³n
docker logs lab_ghosts_driver --tail 50

# DeberÃ­a mostrar:
# [INFO] Starting LLM-powered timeline generation for John Scott
# [INFO] Using LLM: qwen3-coder at https://chat.ai.e-infra.cz/api
# [INFO] Generating query 1/8: Check all employees in Engineering department
# [INFO] Generated query: SELECT * FROM employees WHERE department='Engineering' LIMIT 10;
# [SUCCESS] Timeline generated successfully
```

### 3. Ver Timeline Generado

```bash
# Copiar timeline desde el contenedor
docker cp lab_ghosts_driver:/opt/john_scott_llm/timeline_john_scott_llm.json ./timeline_llm.json

# Ver contenido
cat timeline_llm.json | jq '.TimeLineHandlers[0].TimeLineEvents[] | .Command' | head -20
```

### 4. Verificar EjecuciÃ³n en Tiempo Real

```bash
# Seguir logs en vivo
docker logs lab_ghosts_driver -f

# DeberÃ­a mostrar comandos SSH ejecutÃ¡ndose:
# 2025-11-27 14:00:01|INFO|TIMELINE|Command: ssh labuser@172.30.0.10 "PGPASSWORD=... psql -c \"SELECT AVG(salary) FROM employees GROUP BY department;\""
# Result: department | avg
#         Engineering | 95000.00
#         Marketing   | 75000.00
```

### 5. Conectarse al Contenedor para Debug

```bash
# Entrar al contenedor
docker exec -it lab_ghosts_driver bash

# Ver archivos generados
ls -la /opt/john_scott_llm/
cat /opt/john_scott_llm/timeline_john_scott_llm.json

# Regenerar timeline manualmente
cd /opt/john_scott_llm
python3 generate_timeline_llm.py

# Ver logs de GHOSTS
tail -f /opt/ghosts/bin/logs/*.log
```

### 6. Verificar Queries en la Base de Datos

```bash
# Conectarse a la mÃ¡quina comprometida
ssh -i images/ghosts_driver/john_scott_dummy/id_rsa labuser@localhost -p 2223

# Ver historial de comandos ejecutados por john_scott_llm
history | grep "JOHN_SCOTT_LLM"

# Conectarse directamente a la base de datos
PGPASSWORD=scotty@1 psql -h 172.30.0.3 -U laboratorio -d employees

# Ver tablas disponibles
\dt

# Ejecutar query de ejemplo
SELECT * FROM employees LIMIT 5;
```

## ComparaciÃ³n: Dummy vs LLM

### Similitudes (Lo que comparten)

Ambos modos utilizan la **misma infraestructura base**:

âœ… **Mismo `application.json`** - ConfiguraciÃ³n de GHOSTS idÃ©ntica
âœ… **Mismas credenciales SSH** - Usan `id_rsa` / `id_rsa.pub` compartidas
âœ… **Mismo target SSH** - Ambos se conectan a `labuser@172.30.0.10` (compromised)
âœ… **Misma base de datos** - PostgreSQL en `172.30.0.3:5432` (employees)
âœ… **Mismo usuario DB** - `laboratorio` con contraseÃ±a `scotty@1`
âœ… **Mismo formato timeline** - JSON compatible con GHOSTS Client Universal
âœ… **Mismo HandlerType** - Ambos usan `Command` con bash/ssh
âœ… **Mismo contenedor** - Ejecutan en el mismo `lab/ghosts_driver:latest`
âœ… **Mismo entrypoint** - `entrypoint.sh` detecta el modo via `JOHN_SCOTT_MODE`

**Directorio compartido:**
```
images/ghosts_driver/
â”œâ”€â”€ application.json          â† Compartido por ambos
â”œâ”€â”€ entrypoint.sh             â† Detecta modo (dummy/llm)
â”œâ”€â”€ john_scott_dummy/
â”‚   â”œâ”€â”€ id_rsa                â† Copiado a john_scott_llm/
â”‚   â”œâ”€â”€ id_rsa.pub            â† Copiado a john_scott_llm/
â”‚   â””â”€â”€ timeline.json         â† Timeline estÃ¡tico
â””â”€â”€ john_scott_llm/
    â”œâ”€â”€ id_rsa                â† Copia del dummy
    â”œâ”€â”€ id_rsa.pub            â† Copia del dummy
    â”œâ”€â”€ generate_timeline_llm.py  â† Generador dinÃ¡mico
    â””â”€â”€ timeline_john_scott_llm.json  â† Generado en runtime
```

### Diferencias Clave

| Aspecto | john_scott_dummy | john_scott_llm |
|---------|------------------|----------------|
| **ğŸ¯ GeneraciÃ³n de Queries** | 13 queries SQL **hardcodeadas** en `timeline.json` | Queries **generadas dinÃ¡micamente** por LLM al inicio |
| **ğŸ”„ Variabilidad** | Mismo comportamiento cada vez | Comportamiento **adaptativo** - queries diferentes en cada ejecuciÃ³n |
| **ğŸ“ DefiniciÃ³n de Tareas** | SQL directo en JSON | **Descripciones en lenguaje natural** (ej: "Find average salary by department") |
| **ğŸ”Œ Dependencias Externas** | Ninguna - funciona offline | Requiere **API LLM** (OpenAI-compatible) |
| **âš™ï¸ ConfiguraciÃ³n** | Timeline estÃ¡tico pre-generado | Timeline generado **en tiempo de arranque** |
| **ğŸš€ Tiempo de Inicio** | InstantÃ¡neo (~2 segundos) | +20-30 segundos (llamadas al LLM) |
| **ğŸ“Š Complejidad Queries** | Queries fijas simples/intermedias | Queries **variadas y contextuales** generadas por IA |
| **ğŸ›¡ï¸ Fallback** | N/A - siempre funciona | Queries simples si LLM falla |
| **ğŸ’° Costo** | Gratis | Depende del proveedor LLM (gratis con e-INFRA CZ) |
| **ğŸ”§ Mantenimiento** | Editar JSON manualmente | Editar **descripciones en Python** |

### CuÃ¡ndo Usar Cada Uno

**Usa `john_scott_dummy` cuando:**
- âœ… Necesitas comportamiento predecible y consistente
- âœ… No tienes acceso a API de LLM
- âœ… Quieres arranque rÃ¡pido sin dependencias externas
- âœ… EstÃ¡s en entorno offline/air-gapped
- âœ… El timeline estÃ¡ bien definido y no necesita cambios

**Usa `john_scott_llm` cuando:**
- âœ… Quieres comportamiento mÃ¡s realista y variable
- âœ… Tienes acceso a LLM API (OpenAI, e-INFRA CZ, etc.)
- âœ… Necesitas generar queries complejas sin escribir SQL
- âœ… Quieres simular un desarrollador real que adapta sus queries
- âœ… EstÃ¡s investigando comportamiento adaptativo de NPCs

## Testing Completo

### Test 1: Verificar Modo de OperaciÃ³n

```bash
# Ver quÃ© modo estÃ¡ activo
docker exec lab_ghosts_driver bash -c 'echo "Mode: $JOHN_SCOTT_MODE"'
```

### Test 2: Verificar Conectividad LLM

```bash
# Test manual del API
curl -X POST "https://chat.ai.e-infra.cz/api/v1/chat/completions" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder",
    "messages": [
      {"role": "user", "content": "Generate a SQL query to count all employees"}
    ],
    "temperature": 0.7
  }'
```

### Test 3: Regenerar Timeline con Diferentes Temperaturas

```bash
# Temperatura baja = queries mÃ¡s determinÃ­sticas
docker exec -e LLM_TEMPERATURE=0.2 lab_ghosts_driver bash -c 'cd /opt/john_scott_llm && python3 generate_timeline_llm.py'

# Temperatura alta = queries mÃ¡s creativas
docker exec -e LLM_TEMPERATURE=1.0 lab_ghosts_driver bash -c 'cd /opt/john_scott_llm && python3 generate_timeline_llm.py'

# Comparar diferencias
docker cp lab_ghosts_driver:/opt/john_scott_llm/timeline_john_scott_llm.json ./timeline_temp_test.json
```

### Test 4: Verificar Fallback

```bash
# Simular fallo de LLM (API key invÃ¡lida)
docker exec -e OPENAI_API_KEY=invalid lab_ghosts_driver bash -c 'cd /opt/john_scott_llm && python3 generate_timeline_llm.py'

# DeberÃ­a usar queries fallback simples
docker logs lab_ghosts_driver --tail 20 | grep "fallback"
```

### Test 5: Monitoreo de Actividad

```bash
# Ver actividad en tiempo real
watch -n 2 'docker logs lab_ghosts_driver --tail 5'

# O usar tmux/screen para mÃºltiples vistas:
# Panel 1: logs de GHOSTS
docker logs lab_ghosts_driver -f

# Panel 2: logs del servidor comprometido
docker logs lab_compromised -f

# Panel 3: queries en la base de datos
docker exec -it lab_server bash -c 'tail -f /var/log/postgresql/*.log'
```

## Troubleshooting

### Problema: "405 Method Not Allowed"
**Causa:** URL del API incorrecta (doble barra `/api//chat`)
**SoluciÃ³n:** Verificar que `OPENAI_BASE_URL` no termine en `/`

```bash
# En .env debe ser:
OPENAI_BASE_URL=https://chat.ai.e-infra.cz/api
# NO:
OPENAI_BASE_URL=https://chat.ai.e-infra.cz/api/
```

### Problema: Timeline no se genera
**Causa:** Error en llamada al LLM o permisos
**Debug:**
```bash
docker exec -it lab_ghosts_driver bash
cd /opt/john_scott_llm
python3 generate_timeline_llm.py
# Ver error completo
```

### Problema: Queries SQL invÃ¡lidas
**Causa:** LLM genera SQL mal formateado
**SoluciÃ³n:** Ajustar temperatura o mejorar el prompt del sistema

### Problema: ConexiÃ³n SSH falla
**Causa:** Clave SSH incorrecta o mÃ¡quina comprometida no disponible
**Debug:**
```bash
docker exec lab_ghosts_driver ssh -i /root/.ssh/id_rsa labuser@172.30.0.10 "echo test"
```

## Archivos Importantes

```
john_scott_llm/
â”œâ”€â”€ generate_timeline_llm.py    # Script principal de generaciÃ³n
â”œâ”€â”€ generate_timeline.sh        # Wrapper bash para ejecutar el script
â”œâ”€â”€ requirements.txt            # Dependencias Python (requests)
â”œâ”€â”€ id_rsa                      # Clave SSH privada (copiada de dummy)
â”œâ”€â”€ id_rsa.pub                  # Clave SSH pÃºblica
â””â”€â”€ timeline_john_scott_llm.json  # Timeline generado (creado en runtime)
```

## Ventajas del Enfoque LLM

1. **Comportamiento Adaptativo:** Cada ejecuciÃ³n puede generar queries ligeramente diferentes
2. **Mantenibilidad:** Cambiar el comportamiento editando descripciones en lenguaje natural
3. **Realismo:** Las queries varÃ­an como lo harÃ­a un desarrollador real
4. **Extensibilidad:** FÃ¡cil agregar nuevas tareas sin escribir SQL
5. **Testing:** El LLM puede generar queries de prueba automÃ¡ticamente

## PrÃ³ximos Pasos

- Agregar mÃ¡s tareas de desarrollo realistas
- Integrar anÃ¡lisis de datos con pandas/matplotlib
- Implementar respuestas adaptativas basadas en resultados de queries
- Logging avanzado de actividad del NPC
- IntegraciÃ³n con GHOSTS Shadows para comportamiento mÃ¡s complejo
