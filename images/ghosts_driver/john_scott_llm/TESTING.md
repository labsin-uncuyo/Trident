# Testing Guide for LLM-Driven GHOSTS Workflow

## Overview

This guide covers testing both the **hardcoded** and **LLM-driven** GHOSTS workflows. Both save logs to the same location with identical structure.

## Log Location

All GHOSTS logs are saved to: `./outputs/<RUN_ID>/ghosts/`

Where `<RUN_ID>` is automatically generated when you run `make up` (e.g., `logs_20251215_222402`)

### Log Files Structure

```
./outputs/logs_20251215_222402/ghosts/
â”œâ”€â”€ app.log                    # GHOSTS framework logs (timeline execution, commands)
â”œâ”€â”€ clientupdates.log          # GHOSTS client activity log (detailed command execution)
â””â”€â”€ timeline_generated.json    # (LLM only) The generated timeline used
```

**What's in each log:**

1. **`app.log`** - GHOSTS framework internal logs:
   - Timeline loading and parsing
   - Handler initialization (Bash handler)
   - Command scheduling and execution timestamps
   - Working hours checks
   - Each command that was executed

2. **`clientupdates.log`** - Detailed execution traces:
   - Complete command strings
   - SSH connection details
   - SQL query execution
   - Results and outputs from commands

3. **`timeline_generated.json`** (LLM mode only):
   - The actual timeline.json that was generated by the LLM
   - Useful for debugging what queries were created

## Prerequisites

1. **Initialize infrastructure:**
   ```bash
   make up
   ```
   This creates a RUN_ID and starts core containers (router, server, compromised)

2. **Verify API key** (for LLM mode only):
   ```bash
   grep OPENCODE_API_KEY /home/shared/Trident/.env
   ```
   Should show: `OPENCODE_API_KEY=sk-...`

## Testing Hardcoded Workflow

Uses the fixed timeline from `john_scott_dummy/timeline_john_scott.json`

### Test 1: Single Run (No Repeats)

```bash
make ghosts_psql REPEATS=1 DELAY=2
```

**Expected output:**
- Container starts and executes 6 hardcoded SQL queries
- Completes in ~30-60 seconds
- Logs saved to `./outputs/<RUN_ID>/ghosts/`

**Verify logs:**
```bash
# Check log files exist
ls -lh ./outputs/$(cat ./outputs/.current_run)/ghosts/

# View GHOSTS framework log
tail -50 ./outputs/$(cat ./outputs/.current_run)/ghosts/app.log

# View detailed command execution
tail -100 ./outputs/$(cat ./outputs/.current_run)/ghosts/clientupdates.log

# Count timeline events executed
grep "TIMELINE|" ./outputs/$(cat ./outputs/.current_run)/ghosts/clientupdates.log | wc -l
```

### Test 2: Multiple Repeats

```bash
make ghosts_psql REPEATS=2 DELAY=3
```

**Expected:**
- Runs the same 6 queries twice
- Longer execution time (~120 seconds)
- More events in logs (12 total: 6 queries Ã— 2 repeats)

## Testing LLM-Driven Workflow

Uses OpenCode to generate queries dynamically based on scenario and role.

### Test 1: Basic LLM Run (Developer Routine)

```bash
make ghosts_psql_llm NUM_QUERIES=5 SCENARIO=developer_routine
```

**What happens:**
1. Container starts with `GHOSTS_MODE=llm`
2. Entrypoint calls `generate_timeline.sh`
3. Python script `generate_timeline_llm.py` calls LLM via OpenCode
4. LLM generates 5 SQL queries based on developer_routine scenario
5. Timeline created and saved to `/opt/ghosts/bin/config/timeline.json`
6. GHOSTS executes the generated timeline
7. On completion, logs are copied to `./outputs/<RUN_ID>/ghosts/`

**Expected output:**
```
=== GHOSTS PostgreSQL Workflow (LLM-Driven) ===
Parameters:
  - Number of queries: 5
  - Scenario: developer_routine
  - Database Role: senior_developer_role
  - Delay between commands: 5 seconds

[ghosts_psql_llm] Using existing RUN_ID=logs_YYYYMMDD_HHMMSS
...
âœ“ Container started: lab_ghosts_driver
...
âœ“ Container stopped
âœ“ Logs copied to ./outputs/logs_YYYYMMDD_HHMMSS/ghosts/
âœ“ Generated timeline copied to ./outputs/logs_YYYYMMDD_HHMMSS/ghosts/timeline_generated.json

=== GHOSTS LLM Execution Complete ===
ðŸ“Š Statistics:
  - Timeline events logged: 7
  - Scenario: developer_routine
  - Database Role: senior_developer_role
  - LLM-generated queries: 5
```

**Verify logs:**
```bash
RUN_ID=$(cat ./outputs/.current_run)

# Check all files were created
ls -lh ./outputs/$RUN_ID/ghosts/

# View generated timeline
cat ./outputs/$RUN_ID/ghosts/timeline_generated.json | jq '.TimeLineHandlers[0].TimeLineEvents[].Command' | head -10

# View GHOSTS execution log
tail -50 ./outputs/$RUN_ID/ghosts/app.log

# See actual SQL queries that were run
grep "SELECT" ./outputs/$RUN_ID/ghosts/app.log

# Count events
grep "TIMELINE|" ./outputs/$RUN_ID/ghosts/clientupdates.log | wc -l
```

### Test 2: Different Scenarios

#### HR Audit (admin-focused queries)
```bash
make ghosts_psql_llm NUM_QUERIES=8 SCENARIO=hr_audit ROLE=senior_developer_role DELAY=3
```

#### Performance Review
```bash
make ghosts_psql_llm NUM_QUERIES=10 SCENARIO=performance_review DELAY=4
```

#### Exploratory (random analytical queries)
```bash
make ghosts_psql_llm NUM_QUERIES=6 SCENARIO=exploratory DELAY=2
```

### Test 3: Verify OpenCode Integration

If OpenCode is available in the container:

```bash
# Check if OpenCode works
docker exec lab_ghosts_driver opencode --version

# Test LLM query generation directly
docker exec lab_ghosts_driver bash -c "cd /opt/ghosts/john_scott_llm && python3 llm_query_generator.py --num-queries 3 --scenario developer_routine"
```

If OpenCode is NOT available, the system gracefully falls back to hardcoded queries.

## Monitoring Live Execution

### Watch logs in real-time:

```bash
# Terminal 1: Watch GHOSTS framework logs
docker logs -f lab_ghosts_driver

# Terminal 2: Monitor container status
watch -n 2 "docker ps --filter 'name=lab_ghosts_driver' --format 'table {{.Names}}\t{{.Status}}'"

# Terminal 3: Check log file growth
watch -n 5 "ls -lh ./outputs/\$(cat ./outputs/.current_run)/ghosts/"
```

### Check if timeline was generated (LLM mode):

```bash
docker exec lab_ghosts_driver cat /opt/ghosts/bin/config/timeline.json | jq '.TimeLineHandlers[0].TimeLineEvents | length'
```

## Troubleshooting

### Problem: No logs saved

**Check:**
```bash
# Is RUN_ID set?
cat ./outputs/.current_run

# Did container run?
docker ps -a --filter "name=lab_ghosts_driver"

# Check container logs
docker logs lab_ghosts_driver | tail -50
```

**Solution:**
- Ensure `make up` was run first
- Check entrypoint.sh is copying logs (should see "âœ“ Logs copied" message)

### Problem: LLM generates no queries

**Check:**
```bash
# Is OpenCode installed?
docker exec lab_ghosts_driver which opencode

# Is API key set?
docker exec lab_ghosts_driver bash -c "echo \$OPENCODE_API_KEY"

# Test LLM directly
docker exec lab_ghosts_driver bash -c "cd /opt/ghosts/john_scott_llm && python3 llm_query_generator.py --num-queries 2"
```

**Solution:**
- System falls back to hardcoded queries if LLM fails
- Check `.env` file has `OPENCODE_API_KEY`
- Rebuild container if OpenCode not installed: `docker compose build ghosts_driver`

### Problem: Timeline has wrong format

**Check generated timeline:**
```bash
docker exec lab_ghosts_driver cat /opt/ghosts/bin/config/timeline.json | jq .
```

**Should have structure:**
```json
{
  "Status": "Run",
  "TimeLineHandlers": [{
    "HandlerType": "Bash",
    "Loop": false,
    "TimeLineEvents": [...]
  }]
}
```

### Problem: SQL queries fail

**Check logs for errors:**
```bash
grep -i "error\|fail" ./outputs/$(cat ./outputs/.current_run)/ghosts/app.log
grep -i "error\|psql" ./outputs/$(cat ./outputs/.current_run)/ghosts/clientupdates.log
```

**Common issues:**
- SSH key not configured â†’ run `make ssh_keys`
- Database not ready â†’ check `docker ps` shows `lab_server` healthy
- Wrong credentials â†’ verify in `roles_users.sql`

## Quick Comparison Test

Run both workflows back-to-back to compare:

```bash
# 1. Hardcoded version
make ghosts_psql REPEATS=1 DELAY=2
RUN_ID_HARD=$(cat ./outputs/.current_run)
echo "Hardcoded RUN_ID: $RUN_ID_HARD"

# Wait for completion and check logs
ls -lh ./outputs/$RUN_ID_HARD/ghosts/
grep "TIMELINE|" ./outputs/$RUN_ID_HARD/ghosts/clientupdates.log | wc -l

# 2. Reset and run LLM version
make down
make up
make ghosts_psql_llm NUM_QUERIES=5 SCENARIO=developer_routine
RUN_ID_LLM=$(cat ./outputs/.current_run)
echo "LLM RUN_ID: $RUN_ID_LLM"

# Check logs
ls -lh ./outputs/$RUN_ID_LLM/ghosts/
grep "TIMELINE|" ./outputs/$RUN_ID_LLM/ghosts/clientupdates.log | wc -l

# Compare
echo "=== Comparison ==="
echo "Hardcoded: ./outputs/$RUN_ID_HARD/ghosts/"
echo "LLM:       ./outputs/$RUN_ID_LLM/ghosts/"
```

## Success Criteria

âœ… **Both modes should:**
1. Create log directory: `./outputs/<RUN_ID>/ghosts/`
2. Generate `app.log` with framework logs
3. Generate `clientupdates.log` with command execution details
4. Show timeline events in logs (check with `grep "TIMELINE|"`)
5. Complete without errors
6. Log file sizes > 0 bytes

âœ… **LLM mode should additionally:**
7. Create `timeline_generated.json` with generated queries
8. Show different queries each run (not hardcoded)
9. Queries match the specified SCENARIO
10. Respect the ROLE permissions

## Next Steps After Testing

1. **Analyze logs** to ensure queries are realistic
2. **Compare scenarios** to see if LLM adapts appropriately
3. **Check SLIPS defender** to see if benign traffic is correctly identified
4. **Integrate with attacker workflows** to create mixed traffic scenarios
